# Mathematics of transformers papers

This list collects papers that deal with mathematical properties and models connected to the transformer architecture.

⚠️ This list is under construction. For suggestions and additions please open a pull-request.


## Papers ordered by year

**[Clustering in Deep Stochastic Transformers](https://arxiv.org/abs/2601.21942)** - Lev Fedorov, Michaël E. Sander, Romuald Elie, Pierre Marion, Mathieu Laurière

<details>
<summary>BibTeX</summary>
@misc{fedorov2026clusteringdeepstochastictransformers,
      title={Clustering in Deep Stochastic Transformers}, 
      author={Lev Fedorov and Michaël E. Sander and Romuald Elie and Pierre Marion and Mathieu Laurière},
      year={2026},
      eprint={2601.21942},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2601.21942}, 
}
```
</details>

**[Finite-Time Analysis of Gradient Descent for Shallow Transformers](https://arxiv.org/abs/2601.16514)** - Enes Arda, Semih Cayci, Atilla Eryilmaz
<details>
<summary>BibTeX</summary>
  
@misc{arda2026finitetimeanalysisgradientdescent,
      title={Finite-Time Analysis of Gradient Descent for Shallow Transformers}, 
      author={Enes Arda and Semih Cayci and Atilla Eryilmaz},
      year={2026},
      eprint={2601.16514},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2601.16514}, 
}
```
</details>

**[On the Structure of Stationary Solutions to McKean-Vlasov Equations with Applications to Noisy Transformers]()** - Krishnakumar Balasubramanian, Sayan Banerjee, Philippe Rigollet
<details>
<summary>BibTeX</summary>

```bibtex
@article{balasubramanian2025structure,
  title={On the Structure of Stationary Solutions to McKean-Vlasov Equations with Applications to Noisy Transformers},
  author={Balasubramanian, Krishnakumar and Banerjee, Sayan and Rigollet, Philippe},
  journal={arXiv preprint arXiv:2510.20094},
  year={2025}
}
```
</details>

**[Sparse Transformer Architectures via Regularized Wasserstein Proximal Operator with $L_1$ Prior](https://arxiv.org/abs/2510.16356)** - Fuqun Han, Stanley Osher, Wuchen Li
<details>
<summary>BibTeX</summary>

```bibtex
@misc{han2025sparsetransformerarchitecturesregularized,
      title={Sparse Transformer Architectures via Regularized Wasserstein Proximal Operator with $L_1$ Prior}, 
      author={Fuqun Han and Stanley Osher and Wuchen Li},
      year={2025},
      eprint={2510.16356},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2510.16356}, 
}
```
</details>

**[Transformers through the lens of support-preserving maps between measures](https://arxiv.org/abs/2509.25611)** - Furuya, Takashi and de Hoop, Maarten V and Lassas, Matti
<details>
<summary>BibTeX</summary>

```bibtex
@article{furuya2025transformers,
  title={Transformers through the lens of support-preserving maps between measures},
  author={Furuya, Takashi and de Hoop, Maarten V and Lassas, Matti},
  journal={arXiv preprint arXiv:2509.25611},
  year={2025}
}
```
</details>

**[A multiscale analysis of mean-field transformers in the moderate interaction regime](https://arxiv.org/abs/2509.25040)** - Bruno, Giuseppe and Pasqualotto, Federico and Agazzi, Andrea
<details>
<summary>BibTeX</summary>

```bibtex
@article{bruno2025multiscale,
  title={A multiscale analysis of mean-field transformers in the moderate interaction regime},
  author={Bruno, Giuseppe and Pasqualotto, Federico and Agazzi, Andrea},
  journal={arXiv preprint arXiv:2509.25040},
  year={2025}
}
```
</details>

**[LOCALMAX DYNAMICS FOR ATTENTION IN TRANSFORMERS AND ITS ASYMPTOTIC BEHAVIOR](https://arxiv.org/pdf/2509.15958)** - HENRI CIMETIÈRE, MARIA TERESA CHIRI, AND BAHMAN GHARESIFARD

<details>
<summary>BibTeX</summary>

```bibtex
@article{cimetiere2025localmax,
  title={Localmax dynamics for attention in transformers and its asymptotic behavior},
  author={Cimeti{\`e}re, Henri and Chiri, Maria Teresa and Gharesifard, Bahman},
  journal={arXiv preprint arXiv:2509.15958},
  year={2025}
}
```
</details>

**[A Mathematical Explanation of Transformers for Large Language Models and GPTs](https://arxiv.org/abs/2510.03989)** -
Xue-Cheng Tai, Hao Liu, Lingfeng Li, Raymond H. Chan

<details>
<summary>BibTeX</summary>

```bibtex
@misc{tai2025mathematicalexplanationtransformerslarge,
      title={A Mathematical Explanation of Transformers for Large Language Models and GPTs}, 
      author={Xue-Cheng Tai and Hao Liu and Lingfeng Li and Raymond H. Chan},
      year={2025},
      eprint={2510.03989},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2510.03989}, 
}
```
</details>

**[Attention's forward pass and Frank-Wolfe](https://arxiv.org/abs/2508.09628)** - Albert Alcalde, Borjan Geshkovski, Domènec Ruiz-Balet

<details>
<summary>BibTeX</summary>

```bibtex
@article{alcalde2025attention,
  title={Attention's forward pass and Frank-Wolfe},
  author={Alcalde, Albert and Geshkovski, Borjan and Ruiz-Balet, Dom{\`e}nec},
  journal={arXiv preprint arXiv:2508.09628},
  year={2025}
}
```
</details>

**[Recurrent Self-Attention Dynamics: An Energy-Agnostic Perspective from Jacobians](https://arxiv.org/abs/2505.19458)** - Akiyoshi Tomihari, Ryo Karakida

<details>
<summary>BibTeX</summary>

```bibtex
@misc{tomihari2025recurrentselfattentiondynamicsenergyagnostic,
      title={Recurrent Self-Attention Dynamics: An Energy-Agnostic Perspective from Jacobians}, 
      author={Akiyoshi Tomihari and Ryo Karakida},
      year={2025},
      eprint={2505.19458},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2505.19458}, 
}
```
</details>

**[Analysis of mean-field models arising from self-attention dynamics in transformer architectures with layer normalization](https://arxiv.org/abs/2501.03096)** - Martin Burger, Samira Kabri, Yury Korolev, Tim Roith, Lukas Weigand

<details>
<summary>BibTeX</summary>

```bibtex
@article{burger2025analysis,
  title={Analysis of mean-field models arising from self-attention dynamics in transformer architectures with layer normalization},
  author={Burger, Martin and Kabri, Samira and Korolev, Yury and Roith, Tim and Weigand, Lukas},
  journal={Philosophical Transactions A},
  volume={383},
  number={2298},
  pages={20240233},
  year={2025},
  publisher={The Royal Society}
}
```
</details>

**OT-Transformer: A Continuous-time Transformer Architecture with Optimal Transport Regularization** - Kan, Kelvin and Li, Xingjian and Osher, Stanley (2025)
<details>
<summary>BibTeX</summary>

```bibtex
@article{kan2025ot,    year = {2025},
    journal = {arXiv preprint arXiv:2501.18793},
    author = {Kan, Kelvin and Li, Xingjian and Osher, Stanley},
    title = {OT-Transformer: A Continuous-time Transformer Architecture with Optimal Transport Regularization},}
```
</details>

**The Geometry of Tokens in Internal Representations of Large Language Models** - Viswanathan, Karthik and Gardinazzi, Yuri and Panerai, Giada and Cazzaniga, Alberto and Biagetti, Matteo (2025)
<details>
<summary>BibTeX</summary>

```bibtex
@article{viswanathan2025geometry,    year = {2025},
    journal = {arXiv preprint arXiv:2501.10573},
    author = {Viswanathan, Karthik and Gardinazzi, Yuri and Panerai, Giada and Cazzaniga, Alberto and Biagetti, Matteo},
    title = {The Geometry of Tokens in Internal Representations of Large Language Models},}
```
</details>

**Exact Sequence Classification with Hardmax Transformers** - Alcalde, Albert and Fantuzzi, Giovanni and Zuazua, Enrique (2025)
<details>
<summary>BibTeX</summary>

```bibtex
@article{alcalde2025exact,    year = {2025},
    journal = {arXiv preprint arXiv:2502.02270},
    author = {Alcalde, Albert and Fantuzzi, Giovanni and Zuazua, Enrique},
    title = {Exact Sequence Classification with Hardmax Transformers},}
```
</details>

**A Unified Perspective on the Dynamics of Deep Transformers** - Castin, Val{\'e}rie and Ablin, Pierre and Carrillo, Jos{\'e} Antonio and Peyr{\'e}, Gabriel (2025)
<details>
<summary>BibTeX</summary>

```bibtex
@article{castin2025unified,    year = {2025},
    journal = {arXiv preprint arXiv:2501.18322},
    author = {Castin, Val{\'e}rie and Ablin, Pierre and Carrillo, Jos{\'e} Antonio and Peyr{\'e}, Gabriel},
    title = {A Unified Perspective on the Dynamics of Deep Transformers},}
```
</details>

**A primal-dual framework for transformers and neural networks** - Nguyen, Tan M and Nguyen, Tam and Ho, Nhat and Bertozzi, Andrea L and Baraniuk, Richard G and Osher, Stanley J (2024)
<details>
<summary>BibTeX</summary>

```bibtex
@article{nguyen2024primal,    year = {2024},
    journal = {arXiv:2406.13781},
    author = {Nguyen, Tan M and Nguyen, Tam and Ho, Nhat and Bertozzi, Andrea L and Baraniuk, Richard G and Osher, Stanley J},
    title = {A primal-dual framework for transformers and neural networks},}
```
</details>

[**How Smooth Is Attention?**](https://proceedings.mlr.press/v235/castin24a.html) - Castin, Val\'{e}rie and Ablin, Pierre and Peyr\'{e}, Gabriel (2024)
<details>
<summary>BibTeX</summary>

```bibtex
@inproceedings{castin2024howsmooth,    url = {https://proceedings.mlr.press/v235/castin24a.html},
    pdf = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/castin24a/castin24a.pdf},
    publisher = {PMLR},
    series = {Proceedings of Machine Learning Research},
    volume = {235},
    editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
    year = {2024},
    pages = {5817--5840},
    booktitle = {Proceedings of the 41st International Conference on Machine Learning},
    author = {Castin, Val\'{e}rie and Ablin, Pierre and Peyr\'{e}, Gabriel},
    title = {How Smooth Is Attention?},}
```
</details>

**Clustering in pure-attention hardmax transformers and its role in sentiment analysis** - Alcalde, Albert and Fantuzzi, Giovanni and Zuazua, Enrique (2024)
<details>
<summary>BibTeX</summary>

```bibtex
@article{alcalde2024clustering,    year = {2024},
    journal = {arXiv preprint arXiv:2407.01602},
    author = {Alcalde, Albert and Fantuzzi, Giovanni and Zuazua, Enrique},
    title = {Clustering in pure-attention hardmax transformers and its role in sentiment analysis},}
```
</details>

**Clustering in causal attention masking** - Karagodin, Nikita and Polyanskiy, Yury and Rigollet, Philippe (2024)
<details>
<summary>BibTeX</summary>

```bibtex
@article{karagodin2024clustering,    year = {2024},
    journal = {arXiv preprint arXiv:2411.04990},
    author = {Karagodin, Nikita and Polyanskiy, Yury and Rigollet, Philippe},
    title = {Clustering in causal attention masking},}
```
</details>

**Measure-to-measure interpolation using Transformers** - Geshkovski, Borjan and Rigollet, Philippe and Ruiz-Balet, Dom{\`e}nec (2024)
<details>
<summary>BibTeX</summary>

```bibtex
@article{geshkovski2024measure,    year = {2024},
    journal = {arXiv preprint arXiv:2411.04551},
    author = {Geshkovski, Borjan and Rigollet, Philippe and Ruiz-Balet, Dom{\`e}nec},
    title = {Measure-to-measure interpolation using Transformers},}
```
</details>

**Continuum Attention for Neural Operators** - Calvello, Edoardo and Kovachki, Nikola B and Levine, Matthew E and Stuart, Andrew M (2024)
<details>
<summary>BibTeX</summary>

```bibtex
@article{calvello2024continuum,    year = {2024},
    journal = {arXiv:2406.06486},
    author = {Calvello, Edoardo and Kovachki, Nikola B and Levine, Matthew E and Stuart, Andrew M},
    title = {Continuum Attention for Neural Operators},}
```
</details>

**The Asymptotic Behavior of Attention in Transformers** - Abella, {\'A}lvaro Rodr{\'\i}guez and Silvestre, Jo{\~a}o Pedro and Tabuada, Paulo (2024)
<details>
<summary>BibTeX</summary>

```bibtex
@article{abella2024asymptotic,    year = {2024},
    journal = {arXiv preprint arXiv:2412.02682},
    author = {Abella, {\'A}lvaro Rodr{\'\i}guez and Silvestre, Jo{\~a}o Pedro and Tabuada, Paulo},
    title = {The Asymptotic Behavior of Attention in Transformers},}
```
</details>

**Synchronization on circles and spheres with nonlinear interactions** - Criscitiello, Christopher and Rebjock, Quentin and McRae, Andrew D and Boumal, Nicolas (2024)
<details>
<summary>BibTeX</summary>

```bibtex
@article{criscitiello2024synchronization,    year = {2024},
    journal = {arXiv:2405.18273},
    author = {Criscitiello, Christopher and Rebjock, Quentin and McRae, Andrew D and Boumal, Nicolas},
    title = {Synchronization on circles and spheres with nonlinear interactions},}
```
</details>

**Emergence of meta-stable clustering in mean-field transformer models** - Bruno, Giuseppe and Pasqualotto, Federico and Agazzi, Andrea (2024)
<details>
<summary>BibTeX</summary>

```bibtex
@article{bruno2024emergence,    year = {2024},
    journal = {arXiv preprint arXiv:2410.23228},
    author = {Bruno, Giuseppe and Pasqualotto, Federico and Agazzi, Andrea},
    title = {Emergence of meta-stable clustering in mean-field transformer models},}
```
</details>

**Dynamic metastability in the self-attention model** - Geshkovski, Borjan and Koubbi, Hugo and Polyanskiy, Yury and Rigollet, Philippe (2024)
<details>
<summary>BibTeX</summary>

```bibtex
@article{geshkovski2024dynamic,    year = {2024},
    journal = {arXiv preprint arXiv:2410.06833},
    author = {Geshkovski, Borjan and Koubbi, Hugo and Polyanskiy, Yury and Rigollet, Philippe},
    title = {Dynamic metastability in the self-attention model},}
```
</details>

**A mathematical perspective on transformers** - Geshkovski, Borjan and Letrouit, Cyril and Polyanskiy, Yury and Rigollet, Philippe (2023)
<details>
<summary>BibTeX</summary>

```bibtex
@article{geshkovski2023mathematical,    year = {2023},
    journal = {arXiv:2312.10794},
    author = {Geshkovski, Borjan and Letrouit, Cyril and Polyanskiy, Yury and Rigollet, Philippe},
    title = {A mathematical perspective on transformers},}
```
</details>

**Sinkformers: Transformers with doubly stochastic attention** - Sander, Michael E and Ablin, Pierre and Blondel, Mathieu and Peyr{\'e}, Gabriel (2022)
<details>
<summary>BibTeX</summary>

```bibtex
@inproceedings{sander2022sinkformers,    year = {2022},
    pages = {3515--3530},
    booktitle = {International Conference on Artificial Intelligence and Statistics},
    author = {Sander, Michael E and Ablin, Pierre and Blondel, Mathieu and Peyr{\'e}, Gabriel},
    title = {Sinkformers: Transformers with doubly stochastic attention},}
```
</details>

**Transformers are deep infinite-dimensional non-mercer binary kernel machines** - Wright, Matthew A and Gonzalez, Joseph E (2021)
<details>
<summary>BibTeX</summary>

```bibtex
@article{wright2021transformers,    year = {2021},
    journal = {arXiv:2106.01506},
    author = {Wright, Matthew A and Gonzalez, Joseph E},
    title = {Transformers are deep infinite-dimensional non-mercer binary kernel machines},}
```
</details>

**Redesigning the transformer architecture with insights from multi-particle dynamical systems** - Dutta, Subhabrata and Gautam, Tanya and Chakrabarti, Soumen and Chakraborty, Tanmoy (2021)
<details>
<summary>BibTeX</summary>

```bibtex
@article{dutta2021redesigning,    year = {2021},
    pages = {5531--5544},
    volume = {34},
    journal = {Advances in Neural Information Processing Systems},
    author = {Dutta, Subhabrata and Gautam, Tanya and Chakrabarti, Soumen and Chakraborty, Tanmoy},
    title = {Redesigning the transformer architecture with insights from multi-particle dynamical systems},}
```
</details>

**A mathematical theory of attention** - Vuckovic, James and Baratin, Aristide and Combes, Remi Tachet des (2020)
<details>
<summary>BibTeX</summary>

```bibtex
@article{vuckovic2020mathematical,    year = {2020},
    journal = {arXiv:2007.02876},
    author = {Vuckovic, James and Baratin, Aristide and Combes, Remi Tachet des},
    title = {A mathematical theory of attention},}
```
</details>

**Attention is all you need** - Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin (2017)
<details>
<summary>BibTeX</summary>

```bibtex
@article{vaswani2017attention,    year = {2017},
    journal = {Advances in Neural Information Processing Systems},
    author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    title = {Attention is all you need},}
```
</details>
